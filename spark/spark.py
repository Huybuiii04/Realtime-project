"""
Spark job ƒë·ªÉ x·ª≠ l√Ω data t·ª´ MongoDB v√† l∆∞u v√†o PostgreSQL
ƒê∆∞·ª£c g·ªçi t·ª´ Airflow DAG
"""
import os
import sys
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from datetime import datetime
import logging

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

def create_spark_session():
    """T·∫°o Spark session v·ªõi config MongoDB v√† PostgreSQL"""
    spark = SparkSession.builder \
        .appName("Kafka_MongoDB_to_PostgreSQL_Airflow") \
        .master("local[*]") \
        .config("spark.sql.shuffle.partitions", "4") \
        .config("spark.driver.memory", "2g") \
        .getOrCreate()
    
    logging.info("‚úÖ Spark session created successfully")
    return spark


def read_from_mongodb(spark):
    """ƒê·ªçc data t·ª´ MongoDB collection s·ª≠ d·ª•ng PyMongo"""
    logging.info("üìñ Reading data from MongoDB...")
    
    from pymongo import MongoClient
    import pandas as pd
    import json
    
    try:
        # K·∫øt n·ªëi MongoDB
        # Use host.docker.internal to access host MongoDB from Docker
        mongo_host = os.getenv('MONGO_HOST', 'host.docker.internal')
        mongo_port = os.getenv('MONGO_PORT', '27017')
        client = MongoClient(f"mongodb://{mongo_host}:{mongo_port}/")
        db = client["kafka_data_db"]
        collection = db["product_views_records"]
        
        # L·∫•y t·∫•t c·∫£ documents
        documents = list(collection.find({}))  # Get all documents with _id
        
        if not documents:
            logging.warning("‚ö†Ô∏è No documents found in MongoDB collection")
            # Create an empty dataframe with a simple schema
            from pyspark.sql.types import StructType
            empty_schema = StructType([])
            return spark.createDataFrame([], schema=empty_schema)
        
        # Preprocess documents to handle type inconsistencies
        processed_docs = []
        for doc in documents:
            processed_doc = {}
            for key, value in doc.items():
                if isinstance(value, (dict, list)):
                    # Convert dict or list to JSON string
                    processed_doc[key] = json.dumps(value)
                elif value is None:
                    # Convert None to empty string for consistency
                    processed_doc[key] = ""
                else:
                    # Keep other types as is
                    processed_doc[key] = value
            processed_docs.append(processed_doc)
        
        # Convert to pandas DataFrame first, then to Spark DataFrame
        pdf = pd.DataFrame(processed_docs)
        
        # Fill any remaining NaN values
        pdf = pdf.fillna("")
        
        # Convert to Spark DataFrame
        df = spark.createDataFrame(pdf)
        
        count = df.count()
        logging.info(f"‚úÖ Read {count} records from MongoDB")
        
        client.close()
        return df
        
    except Exception as e:
        logging.error(f"‚ùå Error reading from MongoDB: {str(e)}")
        raise


def process_data_dim_fact(df):
    """Transform data th√†nh Dimension v√† Fact tables theo Star Schema"""
    logging.info("‚öôÔ∏è Processing data into Dimension & Fact tables...")

    # L·∫•y ng√†y m·ªõi nh·∫•t t·ª´ data
    latest_date_row = df.select(F.max(F.substring("local_time", 1, 10)).alias("max_date")).collect()[0]
    report_date = latest_date_row["max_date"] if latest_date_row["max_date"] else datetime.now().strftime("%Y-%m-%d")

    logging.info(f"üìÖ Processing data for date: {report_date}")

    # ============================================================================
    # DIMENSION TABLES
    # ============================================================================

    # 1. DIM_DATE - B·∫£ng dimension ng√†y
    dim_date = (
        df
        # 1. Parse date m·ªôt l·∫ßn t·ª´ local_time
        .withColumn(
            "date",
            F.to_date(F.substring("local_time", 1, 10))
        )
        # 2. L·ªçc nh·ªØng d√≤ng c√≥ date h·ª£p l·ªá
        .filter(F.col("date").isNotNull())
        # 3. T·∫°o c√°c thu·ªôc t√≠nh ng√†y
        .withColumn("year",        F.year("date"))
        .withColumn("month",       F.month("date"))
        .withColumn("day",         F.dayofmonth("date"))
        .withColumn("day_of_week", F.dayofweek("date"))
        .withColumn("week_of_year",F.weekofyear("date"))
        .withColumn("quarter",     F.quarter("date"))
        # 4. T·∫°o date_key d·∫°ng int: yyyyMMdd
        .withColumn(
            "date_key",
            F.date_format("date", "yyyyMMdd").cast("int")
        )
        # 5. Ch·ªâ gi·ªØ c√°c gi√° tr·ªã duy nh·∫•t
        .select(
            "date", "year", "month", "day",
            "day_of_week", "week_of_year", "quarter", "date_key"
        )
        .distinct()
    )

    logging.info(f"üìÖ DIM_DATE: {dim_date.count()} dates")

    # 2. DIM_PRODUCT - B·∫£ng dimension s·∫£n ph·∫©m
    dim_product = (
        df
        .select("product_id")
        .filter(F.col("product_id").isNotNull())
        .distinct()
        .withColumn("product_key", F.monotonically_increasing_id() + 1)
        .withColumn("product_name", F.concat(F.lit("Product "), F.col("product_id")))
        .withColumn("created_date", F.current_date())
        .withColumn("is_active", F.lit(True))
    )

    logging.info(f"üì¶ DIM_PRODUCT: {dim_product.count()} products")

    # 3. DIM_COUNTRY - B·∫£ng dimension qu·ªëc gia
    dim_country = (
        df
        .select("store_id")
        .filter(F.col("store_id").isNotNull())
        .distinct()
        .withColumn("country_key", F.monotonically_increasing_id() + 1)
        .withColumn("country_name", F.concat(F.lit("Country "), F.col("store_id")))
        .withColumn("region", F.lit("Unknown"))
        .withColumn("created_date", F.current_date())
    )

    logging.info(f"üåç DIM_COUNTRY: {dim_country.count()} countries")

    # 4. DIM_REFERRER - B·∫£ng dimension referrer
    dim_referrer = (
        df
        .select("referrer_url")
        .filter(F.col("referrer_url").isNotNull())
        .distinct()
        .withColumn("referrer_key", F.monotonically_increasing_id() + 1)
        .withColumn("referrer_domain",
            F.regexp_extract(F.col("referrer_url"), r"https?://([^/]+)", 1))
        .withColumn("referrer_type",
            F.when(F.col("referrer_url").contains("google"), "Search Engine")
             .when(F.col("referrer_url").contains("facebook"), "Social Media")
             .when(F.col("referrer_url").contains("direct"), "Direct")
             .otherwise("Other"))
        .withColumn("created_date", F.current_date())
    )

    logging.info(f"üîó DIM_REFERRER: {dim_referrer.count()} referrers")

    # 5. DIM_DEVICE - B·∫£ng dimension thi·∫øt b·ªã
    dim_device = (
        df
        .select("device_id")
        .filter(F.col("device_id").isNotNull())
        .distinct()
        .withColumn("device_key", F.monotonically_increasing_id() + 1)
        .withColumn("device_type", F.lit("Unknown"))
        .withColumn("browser_info", F.lit("Unknown"))
        .withColumn("created_date", F.current_date())
    )

    logging.info(f"üì± DIM_DEVICE: {dim_device.count()} devices")

    # ============================================================================
    # FACT TABLE
    # ============================================================================

    # FACT_PRODUCT_VIEWS - B·∫£ng fact ch√≠nh
    fact_product_views = df.filter(
        F.col("local_time").startswith(report_date) #chi xu ly data ng√†y m·ªõi nh·∫•t
    ).alias("main").join(dim_date.alias("dd"),
        F.to_date(F.substring(F.col("main.local_time"), 1, 10)) == F.col("dd.date"),
        "left"
    ).join(dim_product.alias("dp"),
        F.col("main.product_id") == F.col("dp.product_id"),
        "left"
    ).join(dim_country.alias("dc"),
        F.col("main.store_id") == F.col("dc.store_id"),
        "left"
    ).join(dim_referrer.alias("dr"),
        F.col("main.referrer_url") == F.col("dr.referrer_url"),
        "left"
    ).join(dim_device.alias("dv"),
        F.col("main.device_id") == F.col("dv.device_id"),
        "left"
    ).select(
        F.col("dd.date_key"),
        F.col("dp.product_key"),
        F.col("dc.country_key"),
        F.col("dr.referrer_key"),
        F.col("dv.device_key"),
        F.col("main.local_time"),
        F.col("main.device_id")
    ).groupBy(
        "date_key", "product_key", "country_key", "referrer_key", "device_key"
    ).agg(
        F.count("*").alias("view_count"),
        F.countDistinct("device_id").alias("unique_visitors"),
        F.max(F.to_timestamp(F.regexp_replace("local_time", "T", " "))).alias("last_view_time"),
        F.min(F.to_timestamp(F.regexp_replace("local_time", "T", " "))).alias("first_view_time"),
        F.avg(F.unix_timestamp(F.to_timestamp(F.regexp_replace("local_time", "T", " ")))).alias("avg_view_timestamp")
    ).withColumn("view_duration_seconds",
        F.unix_timestamp("last_view_time") - F.unix_timestamp("first_view_time")
    ).withColumn("processed_at", F.current_timestamp())

    logging.info(f"üìä FACT_PRODUCT_VIEWS: {fact_product_views.count()} fact records")

    return {
        "dim_date": dim_date,
        "dim_product": dim_product,
        "dim_country": dim_country,
        "dim_referrer": dim_referrer,
        "dim_device": dim_device,
        "fact_product_views": fact_product_views
    }


def write_to_postgres(reports, jdbc_url, properties):
    """Ghi k·∫øt qu·∫£ Dimension & Fact tables v√†o PostgreSQL"""
    logging.info("üíæ Writing Dimension & Fact tables to PostgreSQL...")

    # Truncate dimension tables first (to avoid foreign key conflicts)
    import psycopg2

    try:
        conn = psycopg2.connect(
            host="postgres",
            port="5432",
            database="postgres",
            user=properties["user"],
            password=properties["password"]
        )
        cursor = conn.cursor()

        # Truncate dimension tables (CASCADE will handle foreign keys)
        truncate_queries = [
            "TRUNCATE TABLE public.dim_date CASCADE;",
            "TRUNCATE TABLE public.dim_product CASCADE;",
            "TRUNCATE TABLE public.dim_country CASCADE;",
            "TRUNCATE TABLE public.dim_referrer CASCADE;",
            "TRUNCATE TABLE public.dim_device CASCADE;"
        ]

        for query in truncate_queries:
            cursor.execute(query)
            logging.info(f"‚úÖ Executed: {query}")

        conn.commit()
        cursor.close()
        conn.close()
        logging.info("‚úÖ All dimension tables truncated")

    except Exception as e:
        logging.error(f"‚ùå Error truncating tables: {str(e)}")
        raise

    # Write Dimension tables (append mode after truncate)
    reports["dim_date"].write \
        .jdbc(url=jdbc_url, table="public.dim_date", mode="append", properties=properties)
    logging.info("‚úÖ DIM_DATE written to PostgreSQL")

    reports["dim_product"].write \
        .jdbc(url=jdbc_url, table="public.dim_product", mode="append", properties=properties)
    logging.info("‚úÖ DIM_PRODUCT written to PostgreSQL")

    reports["dim_country"].write \
        .jdbc(url=jdbc_url, table="public.dim_country", mode="append", properties=properties)
    logging.info("‚úÖ DIM_COUNTRY written to PostgreSQL")

    reports["dim_referrer"].write \
        .jdbc(url=jdbc_url, table="public.dim_referrer", mode="append", properties=properties)
    logging.info("‚úÖ DIM_REFERRER written to PostgreSQL")

    reports["dim_device"].write \
        .jdbc(url=jdbc_url, table="public.dim_device", mode="append", properties=properties)
    logging.info("‚úÖ DIM_DEVICE written to PostgreSQL")

    # Write Fact table (append mode to accumulate data)
    reports["fact_product_views"].write \
        .jdbc(url=jdbc_url, table="public.fact_product_views", mode="append", properties=properties)
    logging.info("‚úÖ FACT_PRODUCT_VIEWS written to PostgreSQL")


def main():
    """Main execution"""
    
    logging.info("üöÄ Starting Spark MongoDB ‚Üí PostgreSQL Processing")
    
    
    # PostgreSQL connection config
    jdbc_url = "jdbc:postgresql://postgres:5432/postgres"
    properties = {
        "user": "postgres",
        "password": "UnigapPostgres@123",
        "driver": "org.postgresql.Driver"
    }
    
    try:
        # Create Spark session
        spark = create_spark_session()
        
        # Read from MongoDB
        df = read_from_mongodb(spark)
        
        if df.count() == 0:
            logging.warning("‚ö†Ô∏è No data found in MongoDB. Skipping processing.")
            return
        
        # Process data into Dim/Fact tables
        reports = process_data_dim_fact(df)
        
        # Write to PostgreSQL
        write_to_postgres(reports, jdbc_url, properties)
        
        
        logging.info("‚úÖ Spark job completed successfully!")
        
        
    except Exception as e:
        logging.error(f"‚ùå Error in Spark job: {e}")
        raise
    finally:
        if 'spark' in locals():
            spark.stop()
            logging.info("üîå Spark session stopped")


if __name__ == "__main__":
    main()
    
    
    
    
    
    
    