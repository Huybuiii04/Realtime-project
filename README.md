# Real-time Product Analytics Pipeline

## ğŸ“‹ Project Overview

A real-time data pipeline that processes product view events from a remote Kafka cluster, stores them in MongoDB, and generates daily analytics reports in PostgreSQL using Apache Spark. The entire workflow is orchestrated by Apache Airflow.

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          DATA PIPELINE ARCHITECTURE                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Remote Kafka    â”‚
â”‚  (Source Data)   â”‚
â”‚  Topic:          â”‚
â”‚  product_views   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ (1) Producer reads remote topic
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Local Kafka     â”‚
â”‚  Cluster         â”‚
â”‚  (3 Brokers)     â”‚
â”‚                  â”‚
â”‚  Topic:          â”‚
â”‚  processed_      â”‚
â”‚  product_view_   â”‚
â”‚  test            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ (2) Consumer reads local topic
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   MongoDB 7.0    â”‚
â”‚                  â”‚
â”‚  Database:       â”‚
â”‚  kafka_data_db   â”‚
â”‚                  â”‚
â”‚  Collection:     â”‚
â”‚  product_views_  â”‚
â”‚  records         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ (3) Spark reads and aggregates
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Apache Spark    â”‚
â”‚  3.5.6           â”‚
â”‚                  â”‚
â”‚  - Cluster Mode  â”‚
â”‚  - 1 Master      â”‚
â”‚  - 2 Workers     â”‚
â”‚                  â”‚
â”‚  Processing:     â”‚
â”‚  - Daily views   â”‚
â”‚  - Top products  â”‚
â”‚  - Top countries â”‚
â”‚  - Top referrers â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ (4) Write analytics results
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PostgreSQL      â”‚
â”‚  16.3            â”‚
â”‚                  â”‚
â”‚  Tables:         â”‚
â”‚  - product_      â”‚
â”‚    views_daily   â”‚
â”‚  - country_      â”‚
â”‚    views_daily   â”‚
â”‚  - referrer_     â”‚
â”‚    views_daily   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

         â–²
         â”‚
         â”‚ (5) Orchestrates entire workflow
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Apache Airflow  â”‚
â”‚  2.10.4          â”‚
â”‚                  â”‚
â”‚  DAG Tasks:      â”‚
â”‚  1. Producer     â”‚
â”‚  2. Consumer     â”‚
â”‚  3. Spark        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”„ Data Flow

### 1. **Producer Stage** (`kafka/producer_app.py`)
- Connects to remote Kafka cluster (147.185.221.24:33415)
- Reads from `product_views` topic
- Transforms and forwards to local Kafka cluster
- Topic: `processed_product_view_test`

### 2. **Consumer Stage** (`kafka/consumer_app.py`)
- Consumes from local Kafka topic
- Batch processing (configurable batch size)
- Stores raw events in MongoDB
- Collection: `product_views_records`

### 3. **Analytics Stage** (`spark/airflow_spark_processor.py`)
- Spark reads from MongoDB
- Dynamic date detection (processes latest date in data)
- Aggregates three reports:
  - **Product Views**: Views per product_id
  - **Country Views**: Views per store_id (country)
  - **Referrer Views**: Views per referrer_url
- Writes results to PostgreSQL

### 4. **Orchestration** (`dags/kafka_pipeline_all_in_one.py`)
- Airflow DAG manages execution order
- Sequential task execution
- Error handling and retry logic

## ğŸ“Š Data Schema

### MongoDB Document Structure
```json
{
  "_id": ObjectId("..."),
  "product_id": "12345",
  "device_id": "device-abc-123",
  "store_id": "US",
  "referrer_url": "https://google.com",
  "local_time": "2025-10-31 03:22:11",
  "remote_ip": "192.168.1.1",
  "timestamp": "2025-10-31T03:22:11Z"
}
```

### PostgreSQL Tables

#### `product_views_daily`
```sql
CREATE TABLE product_views_daily (
    product_id VARCHAR(255) NOT NULL,
    view_count BIGINT,
    unique_visitors BIGINT,
    last_view_time TIMESTAMP,
    report_date DATE NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

#### `country_views_daily`
```sql
CREATE TABLE country_views_daily (
    store_id VARCHAR(255) NOT NULL,
    view_count BIGINT,
    unique_visitors BIGINT,
    report_date DATE NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

#### `referrer_views_daily`
```sql
CREATE TABLE referrer_views_daily (
    referrer_url TEXT NOT NULL,
    view_count BIGINT,
    report_date DATE NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

## ğŸ› ï¸ Technology Stack

| Component | Version | Purpose |
|-----------|---------|---------|
| **Apache Airflow** | 2.10.4 | Workflow orchestration |
| **Apache Kafka** | 3.x | Message streaming (3 brokers) |
| **Apache Spark** | 3.5.6 | Distributed data processing |
| **MongoDB** | 7.0 | Document storage for raw events |
| **PostgreSQL** | 16.3 | Relational storage for analytics |
| **Python** | 3.11 | Application development |
| **Docker** | Latest | Containerization |

## ğŸ“ Project Structure

```
Project--1/
â”œâ”€â”€ airflow/                    # Airflow configuration
â”œâ”€â”€ config/
â”‚   â””â”€â”€ kafka/
â”‚       â””â”€â”€ kafka_server_jaas.conf
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ kafka_pipeline_all_in_one.py  # Main Airflow DAG
â”œâ”€â”€ kafka/
â”‚   â”œâ”€â”€ producer_app.py        # Kafka producer
â”‚   â”œâ”€â”€ consumer_app.py        # Kafka consumer
â”‚   â”œâ”€â”€ requirements.txt       # Kafka dependencies
â”‚   â””â”€â”€ README.md             # Kafka setup guide
â”œâ”€â”€ spark/
â”‚   â”œâ”€â”€ airflow_spark_processor.py  # Main Spark job
â”‚   â”œâ”€â”€ database.py           # Database utilities
â”‚   â”œâ”€â”€ schema.py             # Data schemas
â”‚   â”œâ”€â”€ requirements.txt      # Spark dependencies
â”‚   â”œâ”€â”€ run.py                # Spark runner
â”‚   â”œâ”€â”€ util/
â”‚   â”‚   â”œâ”€â”€ config.py         # Configuration management
â”‚   â”‚   â””â”€â”€ udf_manager.py    # User-defined functions
â”‚   â””â”€â”€ *.md                  # Documentation files
â”œâ”€â”€ logs/                      # Application logs
â”œâ”€â”€ plugins/                   # Airflow plugins
â”œâ”€â”€ docker-compose.yml         # Docker services definition
â””â”€â”€ README.md                 # This file
```

## ğŸš€ Getting Started

### Prerequisites
- Docker & Docker Compose
- Python 3.11+
- Access to remote Kafka cluster credentials

### Installation

1. **Clone the repository**
```bash
git clone <repository-url>
cd Project--1
```

2. **Configure environment**
```bash
# Update Kafka credentials in config/kafka/kafka_server_jaas.conf
# Update connection strings in spark/util/config.py
```

3. **Start services**
```bash
docker-compose up -d
```

4. **Access services**
- Airflow UI: http://localhost:8080
- Spark Master: http://localhost:8081
- MongoDB: localhost:27017
- PostgreSQL: localhost:5432

### Running the Pipeline

1. **Access Airflow UI**
   - Navigate to http://localhost:8080
   - Login with credentials (default: admin/admin)

2. **Enable the DAG**
   - Find `pipeline_v12` (or latest version)
   - Toggle ON

3. **Trigger the DAG**
   - Click "Trigger DAG" button
   - Monitor execution in Graph View

4. **Verify results**
```sql
-- Connect to PostgreSQL
psql -h localhost -U postgres -d postgres

-- Check analytics results
SELECT * FROM product_views_daily ORDER BY view_count DESC LIMIT 10;
SELECT * FROM country_views_daily ORDER BY view_count DESC LIMIT 10;
SELECT * FROM referrer_views_daily ORDER BY view_count DESC LIMIT 10;
```

## ğŸ”§ Configuration

### Kafka Configuration
- **Remote Kafka**: `kafka/producer_app.py`
  ```python
  REMOTE_KAFKA_BROKERS = "147.185.221.24:33415"
  REMOTE_TOPIC = "product_views"
  ```

- **Local Kafka**: `docker-compose.yml`
  ```yaml
  kafka1:27017, kafka2:27017, kafka3:27017
  ```

### MongoDB Configuration
```python
MONGODB_URI = "mongodb://mongo:27017/"
DATABASE_NAME = "kafka_data_db"
COLLECTION_NAME = "product_views_records"
```

### PostgreSQL Configuration
```python
POSTGRES_HOST = "postgres"
POSTGRES_PORT = 5432
POSTGRES_DB = "postgres"
POSTGRES_USER = "postgres"
POSTGRES_PASSWORD = "UnigapPostgres@123"
```

### Spark Configuration
```python
SPARK_MASTER = "spark://spark:7077"
SPARK_EXECUTOR_MEMORY = "1g"
SPARK_EXECUTOR_CORES = 4
```

## ğŸ“ˆ Performance Metrics

### Processing Capabilities
- **Producer**: ~1000 events/second
- **Consumer**: Batch size 1000 (configurable)
- **Spark**: Processes 10,000+ records in ~20 seconds
- **MongoDB**: Write throughput ~5000 docs/second
- **PostgreSQL**: Batch insert ~1000 rows/second

### Resource Requirements
- **Airflow**: 2 CPU, 4GB RAM
- **Kafka (per broker)**: 1 CPU, 1GB RAM
- **Spark Master**: 1 CPU, 1GB RAM
- **Spark Worker**: 4 CPU, 1GB RAM each
- **MongoDB**: 1 CPU, 2GB RAM
- **PostgreSQL**: 1 CPU, 2GB RAM

## ğŸ› Troubleshooting

### Common Issues

1. **Kafka connection failed**
   - Check JAAS configuration in `config/kafka/kafka_server_jaas.conf`
   - Verify network connectivity to remote Kafka

2. **MongoDB write errors**
   - Check MongoDB logs: `docker logs mongo`
   - Verify collection exists and has proper indexes

3. **Spark job fails**
   - Check Spark logs: `docker logs project--1-spark-1`
   - Verify packages: `mongo-spark-connector:10.4.0`, `postgresql:42.7.4`
   - Ensure user permissions: user 'spark' (uid 1001)

4. **PostgreSQL constraints**
   - Ensure NULL values are filtered before insert
   - Check date type casting: `F.to_date(F.lit(report_date))`

### Log Locations
```bash
# Airflow logs
docker logs airflow-webserver
docker logs airflow-scheduler

# Kafka logs
docker logs kafka1
docker logs kafka2
docker logs kafka3

# Spark logs
docker logs project--1-spark-1
docker logs project--1-spark-worker-1
docker logs project--1-spark-worker-2

# MongoDB logs
docker logs mongo

# PostgreSQL logs
docker logs postgres
```

## ğŸ”’ Security Considerations

- âœ… SASL/PLAIN authentication for remote Kafka
- âœ… MongoDB runs without authentication (internal network only)
- âœ… PostgreSQL with password authentication
- âœ… All services run in isolated Docker network
- âš ï¸ Update default passwords in production
- âš ï¸ Enable SSL/TLS for production deployments

## ğŸ“ Development Notes

### Key Implementation Details

1. **Dynamic Date Detection**
   - Spark automatically detects latest date in MongoDB
   - No hardcoded dates in processing logic
   - Handles historical data reprocessing

2. **NULL Value Handling**
   - Filters applied after aggregation
   - Prevents PostgreSQL constraint violations
   - Maintains data quality

3. **Type Safety**
   - Date columns cast with `F.to_date()`
   - Ensures PostgreSQL DATE type compatibility
   - Prevents type mismatch errors

4. **Error Recovery**
   - Airflow retry mechanism
   - Idempotent operations
   - Transaction-safe writes

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test thoroughly
5. Submit a pull request

## ğŸ“„ License

[Add your license information here]

## ğŸ‘¥ Authors

- **Project Team** - Initial work and maintenance

## ğŸ™ Acknowledgments

- Apache Airflow community
- Apache Spark community
- Kafka ecosystem contributors

---

**Last Updated**: November 9, 2025  
**Version**: 1.0.0  
**Status**: Production Ready âœ…
"# Realtime-project" 
