"""
Producer: Bridge data tá»« Remote Kafka â†’ Local Kafka
Consumer: LÆ°u data tá»« Local Kafka â†’ MongoDB
Spark: Transform data thÃ nh Dim/Fact tables â†’ PostgreSQL

Flow: Producer Task â†’ Consumer Task â†’ Spark Task
Sá»­ dá»¥ng import functions thay vÃ¬ subprocess.run Ä‘á»ƒ hiá»‡u quáº£ hÆ¡n
"""
from datetime import datetime, timedelta
import os
import sys
from airflow import DAG
from airflow.operators.python import PythonOperator

# Add kafka path to sys.path Ä‘á»ƒ import Ä‘Æ°á»£c modules
kafka_path = '/opt/airflow/kafka'
if kafka_path not in sys.path:
    sys.path.insert(0, kafka_path)

# Import functions tá»« producer vÃ  consumer
try:
    from producer_app import run_bridge as producer_run_bridge
    from consumer_app import run_consumer as consumer_run_consumer
    print("âœ… Successfully imported producer and consumer functions")
except ImportError as e:
    print(f"âŒ Failed to import functions: {e}")
    raise

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2025, 11, 8),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}


def run_kafka_producer(**context):
    """
    Cháº¡y producer_app.py - import trá»±c tiáº¿p function thay vÃ¬ subprocess
    """
    print("ğŸš€ Starting Kafka Producer - Bridge Remote â†’ Local")

    # Set environment variable Ä‘á»ƒ Python Ä‘á»c .env.docker
    os.environ['ENV_FILE'] = '/opt/airflow/kafka/.env.docker'

    try:
        # Gá»i trá»±c tiáº¿p function tá»« imported module
        producer_run_bridge()
        print("âœ… Producer completed successfully!")
        return 'Producer completed'
    except Exception as e:
        print(f"âŒ Producer failed: {e}")
        raise


def run_kafka_consumer(**context):
    """
    Cháº¡y consumer_app.py - import trá»±c tiáº¿p function thay vÃ¬ subprocess
    """
    print("ğŸš€ Starting Kafka Consumer - Local Kafka â†’ MongoDB")

    # Set environment variable Ä‘á»ƒ Python Ä‘á»c .env.docker
    os.environ['ENV_FILE'] = '/opt/airflow/kafka/.env.docker'

    try:
        # Gá»i trá»±c tiáº¿p function tá»« imported module
        consumer_run_consumer()
        print("âœ… Consumer completed successfully!")
        return 'Consumer completed'
    except Exception as e:
        print(f"âŒ Consumer failed: {e}")
        raise


def run_spark_processing(**context):
    """
    Cháº¡y Spark job Ä‘á»ƒ xá»­ lÃ½ data tá»« MongoDB vÃ  lÆ°u vÃ o PostgreSQL
    """
    spark_script_path = '/opt/bitnami/spark/airflow_spark_processor.py'
    

    print(" Starting Spark Processing - MongoDB â†’ PostgreSQL")
  
    
    # Build spark-submit command (user 'spark' Ä‘Ã£ Ä‘Æ°á»£c táº¡o sáºµn trong container)
    # DÃ¹ng mongo-spark-connector 10.4.0 compatible vá»›i Spark 3.5.x
    spark_command = (
        f"export SPARK_HOME=/opt/bitnami/spark && "
        f"export HOME=/tmp/spark-home && "
        f"mkdir -p /tmp/spark-home && "
        f"/opt/bitnami/spark/bin/spark-submit "
        f"--master spark://spark:7077 "
        f"--packages org.mongodb.spark:mongo-spark-connector_2.12:10.4.0,org.postgresql:postgresql:42.7.4 "
        f"--conf spark.jars.ivy=/tmp/.ivy2 "
        f"--conf spark.mongodb.read.connection.uri=mongodb://mongo:27017/kafka_data_db.product_views_records "
        f"{spark_script_path}"
    )
    
    # Submit Spark job via bash -c
    result = subprocess.run(
        ['docker', 'exec', 'project--1-spark-1', 'bash', '-c', spark_command],
        capture_output=True,
        text=True,
        timeout=7200,  # 2 hours timeout
    )
    
    print("\n SPARK OUTPUT:")
    print(result.stdout)
    
    if result.stderr:
        print("\n SPARK STDERR:")
        print(result.stderr)
    
    if result.returncode != 0:
        raise Exception(f"Spark job failed with return code {result.returncode}")
    
    print("\n Spark processing completed successfully!")
    return 'Spark completed'


with DAG(
    'pipeline_v1',
    default_args=default_args,
    description='Complete Kafka Pipeline: Remote â†’ Local â†’ MongoDB â†’ PostgreSQL (Import Functions)',
    schedule_interval='0 */2 * * *',  # Cháº¡y má»—i 2 giá»
    catchup=False,
    tags=['kafka', 'pipeline', 'producer', 'consumer', 'spark', 'import-functions'],
    max_active_runs=1,  # Chá»‰ cho phÃ©p 1 instance cháº¡y cÃ¹ng lÃºc
) as dag:


    producer_task = PythonOperator(
        task_id='kafka_producer_bridge',
        python_callable=run_kafka_producer,
        execution_timeout=timedelta(hours=2),
    )

    consumer_task = PythonOperator(
        task_id='kafka_consumer_to_mongodb',
        python_callable=run_kafka_consumer,
        execution_timeout=timedelta(hours=2),
    )

    # Task 3: Spark - Xá»­ lÃ½ data tá»« MongoDB vÃ  lÆ°u vÃ o PostgreSQL
    spark_task = PythonOperator(
        task_id='spark_mongodb_to_postgres',
        python_callable=run_spark_processing,
        execution_timeout=timedelta(hours=2),
    )

    # Äá»‹nh nghÄ©a flow: Producer â†’ Consumer â†’ Spark
    producer_task >> consumer_task >> spark_task
