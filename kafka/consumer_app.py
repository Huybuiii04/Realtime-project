# consumer_app.py
import os
import json
import logging
from kafka import KafkaConsumer
from kafka.admin import KafkaAdminClient, NewTopic
from kafka.errors import TopicAlreadyExistsError
from pymongo import MongoClient
from pymongo.errors import ConnectionFailure
from dotenv import load_dotenv
from concurrent.futures import ThreadPoolExecutor

# --- Load .env ---
load_dotenv()

# ---------------- Logging ----------------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)

# ---------------- ENV ----------------
# L∆∞u √Ω: file .env n√™n tr·ªè t·ªõi KAFKA LOCAL
# - N·∫øu ch·∫°y Python tr√™n Windows host:  localhost:9094,localhost:9194,localhost:9294
# - N·∫øu ch·∫°y Python trong container c√πng network Kafka: kafka-0:29092,kafka-1:29092,kafka-2:29092
KAFKA_BROKERS = os.getenv("KAFKA_BROKERS", "kafka-0:9092,kafka-1:9092,kafka-2:9092")
KAFKA_SECURITY_PROTOCOL = os.getenv("KAFKA_SECURITY_PROTOCOL", "SASL_PLAINTEXT").upper()
KAFKA_SASL_MECHANISM = os.getenv("KAFKA_SASL_MECHANISM", "PLAIN")
KAFKA_SASL_USERNAME = os.getenv("KAFKA_SASL_USERNAME", "kafka")
KAFKA_SASL_PASSWORD = os.getenv("KAFKA_SASL_PASSWORD", "UnigapKafka@2024")

DESTINATION_TOPIC = os.getenv("DESTINATION_TOPIC", "destination_topic")
DESTINATION_CONSUMER_GROUP_ID = os.getenv("DESTINATION_CONSUMER_GROUP_ID", "local_product_view_mongo_group")

MONGO_HOST = os.getenv("MONGO_HOST", "localhost")
MONGO_PORT = int(os.getenv("MONGO_PORT", "27017"))
MONGO_DB = os.getenv("MONGO_DB", "kafka_data_db")
MONGO_COLLECTION = os.getenv("MONGO_COLLECTION", "product_views_records")

MAX_WORKERS = int(os.getenv("MAX_WORKERS", "5"))
MAX_MESSAGES = int(os.getenv("MAX_MESSAGES", "100000"))

# Validate env nhanh
missing = []
for key in ["KAFKA_BROKERS", "DESTINATION_TOPIC"]:
    if not globals()[key]:
        missing.append(key)
if missing:
    logging.error(f"Thi·∫øu bi·∫øn m√¥i tr∆∞·ªùng: {', '.join(missing)}. Ki·ªÉm tra file .env!")
    raise SystemExit(1)

BROKER_LIST = [b.strip() for b in KAFKA_BROKERS.split(",") if b.strip()]

# ---------------- Helper Functions ----------------
def create_topic_if_not_exists(topic_name, num_partitions=3, replication_factor=3):
    """T·∫°o topic n·∫øu ch∆∞a t·ªìn t·∫°i."""
    try:
        admin_config = {
            'bootstrap_servers': BROKER_LIST,
            'security_protocol': KAFKA_SECURITY_PROTOCOL,
        }
        
        # Th√™m SASL config n·∫øu c·∫ßn
        if KAFKA_SECURITY_PROTOCOL != 'PLAINTEXT':
            admin_config.update({
                'sasl_mechanism': KAFKA_SASL_MECHANISM,
                'sasl_plain_username': KAFKA_SASL_USERNAME,
                'sasl_plain_password': KAFKA_SASL_PASSWORD
            })
        
        admin_client = KafkaAdminClient(**admin_config)
        
        # T·∫°o topic m·ªõi
        topic = NewTopic(
            name=topic_name,
            num_partitions=num_partitions,
            replication_factor=replication_factor
        )
        
        admin_client.create_topics(new_topics=[topic], validate_only=False)
        logging.info(f" Topic '{topic_name}' created successfully with {num_partitions} partitions and replication factor {replication_factor}")
        admin_client.close()
        
    except TopicAlreadyExistsError:
        logging.info(f"‚Ñπ Topic '{topic_name}' already exists, skipping creation.")
    except Exception as e:
        logging.warning(f" Could not create topic '{topic_name}': {e}")

# ---------------- Kafka Consumer ----------------
def create_kafka_destination_consumer():
    """
    T·∫°o consumer ƒë·ªçc t·ª´ Kafka LOCAL.
    - Kh√¥ng ƒë·∫∑t value_deserializer ·ªü ƒë√¢y ƒë·ªÉ tr√°nh crash n·∫øu d·ªØ li·ªáu kh√¥ng ph·∫£i JSON.
    - Parse JSON ·ªü b∆∞·ªõc x·ª≠ l√Ω message.
    """
    try:
        consumer_kwargs = {
            "bootstrap_servers": BROKER_LIST,
            "security_protocol": KAFKA_SECURITY_PROTOCOL,
            "auto_offset_reset": "earliest",
            "enable_auto_commit": True,
            "group_id": DESTINATION_CONSUMER_GROUP_ID,
            # Tinh ch·ªânh ti√™u th·ª• (c√≥ th·ªÉ ƒë·ªïi cho ph√π h·ª£p)
            "max_poll_records": 100,
            "request_timeout_ms": 30000,
            "session_timeout_ms": 10000,
            "consumer_timeout_ms": 5000,  # Timeout sau 5s n·∫øu kh√¥ng c√≥ message m·ªõi
        }

        # Ch·ªâ g√°n SASL n·∫øu kh√¥ng ph·∫£i PLAINTEXT
        if KAFKA_SECURITY_PROTOCOL != "PLAINTEXT":
            consumer_kwargs.update({
                "sasl_mechanism": KAFKA_SASL_MECHANISM,
                "sasl_plain_username": KAFKA_SASL_USERNAME,
                "sasl_plain_password": KAFKA_SASL_PASSWORD,
            })

        consumer = KafkaConsumer(DESTINATION_TOPIC, **consumer_kwargs)
        logging.info(
            f" Kafka Destination Consumer OK | topic='{DESTINATION_TOPIC}' | group='{DESTINATION_CONSUMER_GROUP_ID}' | brokers={BROKER_LIST}"
        )
        return consumer
    except Exception as e:
        logging.error(f" L·ªói t·∫°o Kafka Destination Consumer: {e}")
        return None

# ---------------- Mongo Client ----------------
def create_mongo_client():
    try:
        client = MongoClient(MONGO_HOST, MONGO_PORT)
        client.admin.command("ping")
        logging.info(f" K·∫øt n·ªëi MongoDB OK | {MONGO_HOST}:{MONGO_PORT}")
        return client
    except ConnectionFailure as e:
        logging.error(f" K·∫øt n·ªëi MongoDB th·∫•t b·∫°i: {e}")
        return None
    except Exception as e:
        logging.error(f" L·ªói MongoDB: {e}")
        return None

# ---------------- Processing ----------------
def process_message(raw_bytes, mongo_collection, message_offset):
    """
    X·ª≠ l√Ω 1 message:
    - Th·ª≠ decode UTF-8
    - Th·ª≠ parse JSON; n·∫øu kh√¥ng ph·∫£i JSON th√¨ l∆∞u d·∫°ng {raw}
    """
    try:
        text = raw_bytes.decode("utf-8", errors="replace")

        try:
            doc = json.loads(text)
        except json.JSONDecodeError:
            doc = {"raw": text}

        result = mongo_collection.insert_one(doc)
        logging.info(f" Saved offset={message_offset} _id={result.inserted_id}")
    except Exception as mongo_err:
        logging.error(f" L·ªói l∆∞u Mongo (offset {message_offset}): {mongo_err}")

# ---------------- Main loop ----------------
def run_consumer():
    # T·∫°o topic tr∆∞·ªõc n·∫øu ch∆∞a t·ªìn t·∫°i
    logging.info(f"üîç Checking if topic '{DESTINATION_TOPIC}' exists...")
    create_topic_if_not_exists(DESTINATION_TOPIC, num_partitions=3, replication_factor=3)
    
    consumer = create_kafka_destination_consumer()
    mongo_client = create_mongo_client()

    if not consumer or not mongo_client:
        logging.error(" Kh√¥ng kh·ªüi t·∫°o ƒë∆∞·ª£c consumer ho·∫∑c MongoDB client. Tho√°t.")
        return

    mongo_collection = mongo_client[MONGO_DB][MONGO_COLLECTION]

    logging.info(
        f" Consume t·ª´ '{DESTINATION_TOPIC}' v·ªõi {MAX_WORKERS} worker ‚Üí Mongo ({MONGO_DB}.{MONGO_COLLECTION})"
    )
    logging.info(f" Max messages to process: {MAX_MESSAGES}")
    logging.info("Nh·∫•n Ctrl+C ƒë·ªÉ d·ª´ng...")

    message_count = 0
    try:
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            try:
                for message in consumer:
                    if message_count >= MAX_MESSAGES:
                        logging.info(f" Reached maximum messages limit: {MAX_MESSAGES}")
                        break
                        
                    message_offset = message.offset
                    logging.info(f" Nh·∫≠n offset {message_offset} ‚Üí ƒë·∫©y v√†o thread pool")
                    executor.submit(process_message, message.value, mongo_collection, message_offset)
                    message_count += 1
                    
            except StopIteration:
                logging.info(f" Consumer timeout - no more messages available. Processed {message_count} messages.")
            except KeyboardInterrupt:
                logging.info(" Ng∆∞·ªùi d√πng ng·∫Øt. ƒêang ch·ªù c√°c t√°c v·ª• d·ªü dang...")
            except Exception as e:
                logging.error(f" L·ªói v√≤ng l·∫∑p consumer: {e}")
        
        # ThreadPoolExecutor context exits here, ensuring all tasks complete
        logging.info(" T·∫•t c·∫£ worker threads ƒë√£ ho√†n th√†nh.")
        
    finally:
        try:
            consumer.close()
        except Exception:
            pass
        try:
            mongo_client.close()
            logging.info(" ƒê√£ ƒë√≥ng MongoDB client.")
        except Exception:
            pass
        logging.info(f" Consumer d·ª´ng an to√†n. Processed {message_count} messages.")

if __name__ == "__main__":
    run_consumer()
